{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_from_disk\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['rating', 'title', 'text', 'product_name'],\n",
      "        num_rows: 350764\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['rating', 'title', 'text', 'product_name'],\n",
      "        num_rows: 175382\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['rating', 'title', 'text', 'product_name'],\n",
      "        num_rows: 175382\n",
      "    })\n",
      "})\n",
      "['train', 'test', 'valid']\n"
     ]
    }
   ],
   "source": [
    "amazon_train_test_valid_dataset_final= ds = load_from_disk('/home/oviya/NLP_Project/amazon_product_dataset')\n",
    "print(amazon_train_test_valid_dataset_final)\n",
    "datasets=[datasets for datasets in amazon_train_test_valid_dataset_final]\n",
    "print(datasets)\n",
    "product_name_list=[item['product_name']  for dicts  in datasets for item in amazon_train_test_valid_dataset_final[dicts]  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rating': 5.0,\n",
       " 'title': 'Perfect',\n",
       " 'text': 'Perfect',\n",
       " 'product_name': 'LUBEX UV LED Nail Lamp Nail Dryer Light, 54W Professional Nail Curing Lamp Nail Light for Gel Polish with 30s/60s/99s Timer, Gel Nail Machine with Auto Sensor LCD Screen with Anti-UV Gloves Offered'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_train_test_valid_dataset_final['train'][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_products(products, search_term):\n",
    "    search_term_lower = search_term.lower()\n",
    "    filtered_products = [\n",
    "        product for product in products \n",
    "        if search_term_lower in product.lower() or \n",
    "        any(word.lower().startswith(search_term_lower) for word in product.split())\n",
    "    ]\n",
    "    return filtered_products\n",
    "\n",
    "def display_products(products):\n",
    "    page = 0\n",
    "    selected_product = None\n",
    "\n",
    "    def show_page(page):\n",
    "        clear_output(wait=True)\n",
    "        start = page * 10\n",
    "        end = min(start + 10, len(products))\n",
    "        products_text = \"Press 0 to move to next page, or enter the number to select a product:\\n\\n\"\n",
    "        for j in range(start, end):\n",
    "            products_text += f\"{j-start+1}- {products[j]}\\n\"\n",
    "        \n",
    "        text_area = widgets.Textarea(\n",
    "            value=products_text,\n",
    "            disabled=True,\n",
    "            layout=widgets.Layout(width='100%', height='300px')\n",
    "        )\n",
    "        \n",
    "        button_prev = widgets.Button(description=\"Previous\")\n",
    "        button_next = widgets.Button(description=\"Next\")\n",
    "        \n",
    "        def on_button_clicked(b):\n",
    "            nonlocal page\n",
    "            if b.description == \"Previous\":\n",
    "                page = max(0, page - 1)\n",
    "            else:\n",
    "                page = min((len(products) - 1) // 10, page + 1)\n",
    "            show_page(page)\n",
    "        \n",
    "        button_prev.on_click(on_button_clicked)\n",
    "        button_next.on_click(on_button_clicked)\n",
    "        \n",
    "        choice_input = widgets.IntText(\n",
    "            value=0,\n",
    "            description='Choice:',\n",
    "            disabled=False\n",
    "        )\n",
    "        \n",
    "        def on_choice_submit(b):\n",
    "            nonlocal selected_product\n",
    "            choice = choice_input.value\n",
    "            if 1 <= choice <= 10 and start + choice - 1 < len(products):\n",
    "                selected_product = products[start + choice - 1]\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Selected product: {selected_product}\")\n",
    "            elif choice == 0:\n",
    "                show_page(page + 1)\n",
    "            else:\n",
    "                print(\"Invalid choice. Please try again.\")\n",
    "        \n",
    "        submit_button = widgets.Button(description=\"Submit\")\n",
    "        submit_button.on_click(on_choice_submit)\n",
    "        \n",
    "        display(widgets.VBox([\n",
    "            text_area,\n",
    "            widgets.HBox([button_prev, button_next]),\n",
    "            widgets.HBox([choice_input, submit_button])\n",
    "        ]))\n",
    "    \n",
    "    show_page(0)\n",
    "\n",
    "\n",
    "search_term = input(\"Enter a search term: \")\n",
    "filtered_products = filter_products(product_name_list, search_term)\n",
    "\n",
    "print(product_name_list)\n",
    "\n",
    "if filtered_products:\n",
    "    print(f\"Found {len(filtered_products)} products. Displaying results:\")\n",
    "    display_products(filtered_products)\n",
    "else:\n",
    "    print(\"No products found matching your search term.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c03cc93cbc4fde8e98f866760c04dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='', description='Search:', placeholder='Enter search term'), Button(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected product outside the function: None\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Global variable to store the selected product\n",
    "global_selected_product = None\n",
    "\n",
    "def filter_products(products, search_term):\n",
    "    search_term_lower = search_term.lower()\n",
    "    filtered_products = [\n",
    "        product for product in products \n",
    "        if search_term_lower in product.lower() or \n",
    "        any(word.lower().startswith(search_term_lower) for word in product.split())\n",
    "    ]\n",
    "    return filtered_products\n",
    "\n",
    "def display_products(products):\n",
    "    global global_selected_product\n",
    "    page = 0\n",
    "    filtered_products = products\n",
    "\n",
    "    def show_page(page):\n",
    "        clear_output(wait=True)\n",
    "        start = page * 10\n",
    "        end = min(start + 10, len(filtered_products))\n",
    "        \n",
    "        products_text = \"Press 0 to move to next page, or enter the number to select a product:\\n\\n\"\n",
    "        for j in range(start, end):\n",
    "            products_text += f\"{j-start+1}- {filtered_products[j]}\\n\"\n",
    "        \n",
    "        text_area = widgets.Textarea(\n",
    "            value=products_text,\n",
    "            disabled=True,\n",
    "            layout=widgets.Layout(width='100%', height='300px')\n",
    "        )\n",
    "        \n",
    "        button_prev = widgets.Button(description=\"Previous\")\n",
    "        button_next = widgets.Button(description=\"Next\")\n",
    "        \n",
    "        def on_button_clicked(b):\n",
    "            nonlocal page\n",
    "            if b.description == \"Previous\":\n",
    "                page = max(0, page - 1)\n",
    "            else:\n",
    "                page = min((len(filtered_products) - 1) // 10, page + 1)\n",
    "            show_page(page)\n",
    "        \n",
    "        button_prev.on_click(on_button_clicked)\n",
    "        button_next.on_click(on_button_clicked)\n",
    "        \n",
    "        choice_input = widgets.IntText(\n",
    "            value=0,\n",
    "            description='Choice:',\n",
    "            disabled=False\n",
    "        )\n",
    "        \n",
    "        def on_choice_submit(b):\n",
    "            global global_selected_product\n",
    "            choice = choice_input.value\n",
    "            if 1 <= choice <= 10 and start + choice - 1 < len(filtered_products):\n",
    "                global_selected_product = filtered_products[start + choice - 1]\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Selected product: {global_selected_product}\")\n",
    "            elif choice == 0:\n",
    "                show_page(page + 1)\n",
    "            else:\n",
    "                print(\"Invalid choice. Please try again.\")\n",
    "        \n",
    "        submit_button = widgets.Button(description=\"Submit\")\n",
    "        submit_button.on_click(on_choice_submit)\n",
    "        \n",
    "        search_input = widgets.Text(\n",
    "            value='',\n",
    "            placeholder='Enter search term',\n",
    "            description='Search:',\n",
    "            disabled=False\n",
    "        )\n",
    "        \n",
    "        def on_search_submit(b):\n",
    "            nonlocal filtered_products, page\n",
    "            search_term = search_input.value\n",
    "            filtered_products = filter_products(products, search_term)\n",
    "            page = 0\n",
    "            show_page(page)\n",
    "        \n",
    "        search_button = widgets.Button(description=\"Search\")\n",
    "        search_button.on_click(on_search_submit)\n",
    "        \n",
    "        display(widgets.VBox([\n",
    "            widgets.HBox([search_input, search_button]),\n",
    "            text_area,\n",
    "            widgets.HBox([button_prev, button_next]),\n",
    "            widgets.HBox([choice_input, submit_button])\n",
    "        ]))\n",
    "    \n",
    "    show_page(0)\n",
    "\n",
    "# Assuming product_name_list is defined elsewhere in your code\n",
    "# product_name_list = [...]\n",
    "\n",
    "display_products(product_name_list)\n",
    "\n",
    "# You can now access the selected product from anywhere in your code\n",
    "print(f\"Selected product outside the function: {global_selected_product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews of the product :\n",
      "1.My granddaughter loved them\n",
      "2.WOW great price.\n",
      "3.I love this item so far, very cute of design, all colors are beautiful, the quality is great as well.I use it everyday,not too tie not too loose, it is perfect!\n",
      "4.Cheap, one of the clips broke right away on the spot. And it tough on hair , it pulls hair, because the surface of the clips is not smooth\n",
      "5.Bueno\n",
      "6.So much prettier in person!! I love them! Simple clip and pin makes it easy to use.\n",
      "7.Nice color and nice size\n",
      "8.Very elegant and nice to wear to different events. The way it looks in the picture, is the same way it is. Very very nice. And it’s sturdy as well, I dropped them and they stay together.\n",
      "9.Adorable but hard to stay in the hair.\n",
      "Positive Feedback: 98.82%\n",
      "Neutral Feedback:  0.92%\n",
      "Neutral Feedback: 0.26%\n",
      "The summarized review is :  Adorable but hard to stay in the hair, it’s a bit tough on hair, but worth the price. I love this item so far, very cute of design, all colors are beautiful, the quality is great as well. So much prettier in person\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "tokenizer =AutoTokenizer.from_pretrained('final_bart_model')\n",
    "\n",
    "def summarize_reviews1(product_name):\n",
    "    # Filter reviews based on product name\n",
    "    reviews = []\n",
    "    ratings ={\"positive\":0,\n",
    "              \"negative\":0,\n",
    "              \"neutral\":0}\n",
    "    for item in amazon_train_test_valid_dataset_final['train']:\n",
    "        if item.get('product_name') == product_name:\n",
    "            reviews.append(item['text'])\n",
    "            current_rating=item['rating']\n",
    "            if current_rating == 5 or current_rating == 4:\n",
    "                ratings['positive']+=1\n",
    "            elif current_rating == 1 or current_rating == 2:\n",
    "                ratings['negative']+=1\n",
    "            elif current_rating == 3:\n",
    "                ratings['neutral']+=1\n",
    "    \n",
    "    # print(\"ratings\",ratings)\n",
    "    reviews = [item['text'] for item in amazon_train_test_valid_dataset_final['train'] if item.get('product_name') == product_name]\n",
    "    print(\"Reviews of the product :\")\n",
    "    for i in range(len(reviews)):\n",
    "        print(f\"{i+1}.{reviews[i]}\")\n",
    "    # title = [item['title'] for item in amazon_train_test_valid_dataset_final['train'] if item.get('product_name') == product_name]\n",
    "    output=[]\n",
    "    if not reviews:\n",
    "        return \"No reviews found for this product.\"\n",
    "    # Join all reviews into a single string\n",
    "    reviews_text = \" \".join(reviews)\n",
    "    # title_text=\" \".join(title)\n",
    "    # print(\"reviews\",reviews_text)\n",
    "    pipe = pipeline('summarization', model='final_bart_model', tokenizer=tokenizer)\n",
    "    gen_kwargs= {'length_penalty':0.8,'num_beams':8 ,'max_length':128}\n",
    "    # summary = pipe(reviews_text,do_sample=False)[0]['summary_text']\n",
    "    # # summary = pipe(reviews_text,**gen_kwargs)\n",
    "    if len(reviews_text) > 1024:\n",
    "        max_chunk_size = 1024  # Adjust this value based on the model's maximum token limit\n",
    "        chunks = [reviews_text[i:i+max_chunk_size] for i in range(0, len(reviews_text), max_chunk_size)]\n",
    "        summaries = [pipe(chunk,**gen_kwargs, do_sample=False)[0]['summary_text'] for chunk in chunks]\n",
    "        summaryall = \" \".join(summaries)\n",
    "        # summary=pipe(summaryall,**gen_kwargs)[0]['summary_text']\n",
    "        output.append(reviews_text) #here\n",
    "        output.append(summaryall)\n",
    "    # if len(summary)\n",
    "    else:\n",
    "        output.append(reviews_text)\n",
    "        summary = pipe(reviews_text,**gen_kwargs)[0]['summary_text']\n",
    "        output.append(summary)\n",
    "    return output ,ratings\n",
    "\n",
    "data=summarize_reviews1(global_selected_product)\n",
    "summary_of_reviews=data[0]\n",
    "# print(\"summary: /n\",summary_of_reviews[1])\n",
    "scores=sentiment_analyser(summary_of_reviews[1])\n",
    "\n",
    "sc_list=scores[1]\n",
    "postive=sc_list[2]*100\n",
    "neutral=sc_list[1]*100\n",
    "negative=sc_list[0]*100\n",
    "\n",
    "postive = round(postive,2)\n",
    "neutral = round(neutral,2)\n",
    "negative = round(negative,2)\n",
    "\n",
    "print(f\"Positive Feedback: {postive:.2f}%\")\n",
    "print(f\"Neutral Feedback:  {neutral:.2f}%\")\n",
    "print(f\"Neutral Feedback: {negative:.2f}%\")\n",
    "print(\"The summarized review is : \",summary_of_reviews[1])\n",
    "\n",
    "\n",
    "# print(\"reviews\",summary_of_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#setiment analyzer with roberta\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "MODEL=f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer1 =AutoTokenizer.from_pretrained(MODEL)\n",
    "model1=AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyser(summary):\n",
    "    input_text=tokenizer1(summary, return_tensors='pt',truncation=True ,max_length=512)\n",
    "    output=model1(**input_text)\n",
    "    scores=output[0][0].detach().numpy()\n",
    "    scores= softmax(scores)\n",
    "    scores_dict={\n",
    "        'roberta_neg':scores[0],\n",
    "        'roberta_neu':scores[1],\n",
    "        'roberta_pos':scores[2]\n",
    "    }\n",
    "    return scores_dict, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roberta_neg': np.float32(0.40362224), 'roberta_neu': np.float32(0.47103682), 'roberta_pos': np.float32(0.12534092)}\n",
      "[0.40362224 0.47103682 0.12534092]\n",
      "1\n",
      "Precision: 0.25\n",
      "Recall: 0.5\n",
      "F1 Score: 0.3333333333333333\n",
      "Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy\n",
    "trail_dict =sentiment_analyser(summary_of_reviews[0])\n",
    "print(trail_dict[0])\n",
    "print(trail_dict[1])\n",
    "\n",
    "# dict=numpy.array([0.8,0.1,0.2])\n",
    "#\n",
    " # 0: negative, 1: neutral, 2: positive\n",
    "predicted_label =  trail_dict[1].argmax()\n",
    "\n",
    "print(predicted_label)\n",
    "ratings=data[1]\n",
    "predicted_sentiment = ['negative', 'neutral', 'positive'][predicted_label]\n",
    "true_labels = ['positive'] * ratings['positive'] + ['neutral'] * ratings['neutral'] + ['negative'] * ratings['negative']\n",
    "predicted_labels = [predicted_sentiment] * len(true_labels)  # Assuming the same sentiment for all reviews\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings {'positive': 1, 'negative': 0, 'neutral': 1}\n",
      "senmitent_summary ({'roberta_neg': np.float32(0.21331248), 'roberta_neu': np.float32(0.4581311), 'roberta_pos': np.float32(0.32855636)}, array([0.21331248, 0.4581311 , 0.32855636], dtype=float32))\n",
      "senmitent_reviews ({'roberta_neg': np.float32(0.40362224), 'roberta_neu': np.float32(0.47103682), 'roberta_pos': np.float32(0.12534092)}, array([0.40362224, 0.47103682, 0.12534092], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "#Evaluation for sentiment analyser\n",
    "\n",
    "senmitent_summary=sentiment_analyser(summary_of_reviews[1])\n",
    "senmitent_reviews=sentiment_analyser(summary_of_reviews[0])\n",
    "\n",
    "print(\"ratings\",data[1])\n",
    "print('senmitent_summary',senmitent_summary)\n",
    "print('senmitent_reviews',senmitent_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from bert_score) (2.3.1)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from bert_score) (2.2.2)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from bert_score) (4.41.2)\n",
      "Requirement already satisfied: numpy in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from bert_score) (2.0.0)\n",
      "Requirement already satisfied: requests in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from bert_score) (4.66.4)\n",
      "Requirement already satisfied: matplotlib in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from bert_score) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from bert_score) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from pandas>=1.0.1->bert_score) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from pandas>=1.0.1->bert_score) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from pandas>=1.0.1->bert_score) (2024.1)\n",
      "Requirement already satisfied: filelock in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (3.15.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert_score) (12.5.40)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers>=3.0.0->bert_score) (0.23.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers>=3.0.0->bert_score) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers>=3.0.0->bert_score) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers>=3.0.0->bert_score) (0.4.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from matplotlib->bert_score) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from matplotlib->bert_score) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from matplotlib->bert_score) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from matplotlib->bert_score) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from matplotlib->bert_score) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests->bert_score) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests->bert_score) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests->bert_score) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests->bert_score) (2024.6.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/oviya/anaconda3/envs/nlp/lib/python3.12/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bert_score\n",
      "Successfully installed bert_score-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d082e411eb754fffbe399234236fd775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486f44e87885498ca22cab951feef7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.04 seconds, 26.64 sentences/sec\n",
      "0.9714126586914062\n",
      "BERTScore Precision: 0.9714126586914062\n",
      "BERTScore Recall: 0.9090476632118225\n",
      "BERTScore F1: 0.939195990562439\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Original text and generated summary\n",
    "original_text = summary_of_reviews[0]\n",
    "generated_summary = summary_of_reviews[1]\n",
    "\n",
    "# Calculate BERTScore\n",
    "P, R, F1 = score([generated_summary], [original_text], lang='en', verbose=True)\n",
    "\n",
    "# Display the BERTScore results\n",
    "print(P.item())\n",
    "print(f\"BERTScore Precision: {P.mean().item()}\")\n",
    "print(f\"BERTScore Recall: {R.mean().item()}\")\n",
    "print(f\"BERTScore F1: {F1.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "true_labels = np.array([0.18457678, 0.27753708, 0.53788614])\n",
    "predicted_labels = np.array([0.1342161,0.34502518, 0.5207587])\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "# Print the F1 score\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coverage 0.2857142857142857\n",
      "redundancy 0.23809523809523814\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics for summarizer\n",
    "def calculate_coverage(original_text, summary):\n",
    "    original_words = set(original_text.split())\n",
    "    summary_words = set(summary.split())\n",
    "    coverage = len(summary_words & original_words) / len(original_words)\n",
    "    return coverage\n",
    "\n",
    "def calculate_redundancy(summary):\n",
    "    words = summary.split()\n",
    "    unique_words = set(words)\n",
    "    redundancy = 1 - (len(unique_words) / len(words))\n",
    "    return redundancy\n",
    "\n",
    "\n",
    "trial1=calculate_coverage(summary_of_reviews[0],summary_of_reviews[1])\n",
    "redundent= calculate_redundancy(summary_of_reviews[1])\n",
    "\n",
    "print(\"coverage\",trial1)\n",
    "print(\"redundancy\",redundent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
